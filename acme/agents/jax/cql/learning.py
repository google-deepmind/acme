# Copyright 2018 DeepMind Technologies Limited. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""CQL learner implementation."""

import time
from typing import Any, Dict, Iterator, List, NamedTuple, Optional, Tuple

import jax
import jax.numpy as jnp
import optax

import acme
from acme import types
from acme.agents.jax.cql.networks import CQLNetworks, apply_and_sample_n
from acme.jax import networks as networks_lib
from acme.jax import utils
from acme.utils import counting, loggers

_CQL_COEFFICIENT_MAX_VALUE = 1e6
_CQL_GRAD_CLIPPING_VALUE = 40


class TrainingState(NamedTuple):
    """Contains training state for the learner."""

    policy_optimizer_state: optax.OptState
    critic_optimizer_state: optax.OptState
    policy_params: networks_lib.Params
    critic_params: networks_lib.Params
    target_critic_params: networks_lib.Params
    key: networks_lib.PRNGKey

    # Optimizer and value of the alpha parameter from SAC (entropy temperature).
    # These fields are only used with an adaptive coefficient (when
    # fixed_entropy_coefficeint is None in the CQLLearner)
    alpha_optimizer_state: Optional[optax.OptState] = None
    log_sac_alpha: Optional[networks_lib.Params] = None

    # Optimizer and value of the alpha parameter from CQL (regularization
    # coefficient).
    # These fields are only used with an adaptive coefficient (when
    # fixed_cql_coefficiennt is None in the CQLLearner)
    cql_optimizer_state: Optional[optax.OptState] = None
    log_cql_alpha: Optional[networks_lib.Params] = None

    steps: int = 0


class CQLLearner(acme.Learner):
    """CQL learner.

  Learning component of the Conservative Q-Learning algorithm from
  [Kumar et al., 2020] https://arxiv.org/abs/2006.04779.
  """

    _state: TrainingState

    def __init__(
        self,
        batch_size: int,
        networks: CQLNetworks,
        random_key: networks_lib.PRNGKey,
        demonstrations: Iterator[types.Transition],
        policy_optimizer: optax.GradientTransformation,
        critic_optimizer: optax.GradientTransformation,
        tau: float = 0.005,
        fixed_cql_coefficient: Optional[float] = None,
        cql_lagrange_threshold: Optional[float] = None,
        cql_num_samples: int = 10,
        num_sgd_steps_per_step: int = 1,
        reward_scale: float = 1.0,
        discount: float = 0.99,
        fixed_entropy_coefficient: Optional[float] = None,
        target_entropy: Optional[float] = 0,
        num_bc_iters: int = 50_000,
        counter: Optional[counting.Counter] = None,
        logger: Optional[loggers.Logger] = None,
    ):
        """Initializes the CQL learner.

    Args:
      batch_size: batch size.
      networks: CQL networks.
      random_key: a key for random number generation.
      demonstrations: an iterator over training data.
      policy_optimizer: the policy optimizer.
      critic_optimizer: the Q-function optimizer.
      tau: target smoothing coefficient.
      fixed_cql_coefficient: the value for cql coefficient. If None, an adaptive
        coefficient will be used.
      cql_lagrange_threshold: a threshold that controls the adaptive loss for
        the cql coefficient.
      cql_num_samples: number of samples used to compute logsumexp(Q) via
        importance sampling.
      num_sgd_steps_per_step: how many gradient updated to perform per batch.
        batch is split into this many smaller batches, thus should be a multiple
        of num_sgd_steps_per_step
      reward_scale: reward scale.
      discount: discount to use for TD updates.
      fixed_entropy_coefficient: coefficient applied to the entropy bonus. If
        None, an adaptative coefficient will be used.
      target_entropy: Target entropy when using adapdative entropy bonus.
      num_bc_iters: Number of BC steps for actor initialization.
      counter: counter object used to keep track of steps.
      logger: logger object to be used by learner.
    """
        self._num_bc_iters = num_bc_iters
        adaptive_entropy_coefficient = fixed_entropy_coefficient is None
        action_spec = networks.environment_specs.actions
        if adaptive_entropy_coefficient:
            # sac_alpha is the temperature parameter that determines the relative
            # importance of the entropy term versus the reward.
            log_sac_alpha = jnp.asarray(0.0, dtype=jnp.float32)
            alpha_optimizer = optax.adam(learning_rate=3e-4)
            alpha_optimizer_state = alpha_optimizer.init(log_sac_alpha)
        else:
            if target_entropy:
                raise ValueError(
                    "target_entropy should not be set when "
                    "fixed_entropy_coefficient is provided"
                )

        adaptive_cql_coefficient = fixed_cql_coefficient is None
        if adaptive_cql_coefficient:
            log_cql_alpha = jnp.asarray(0.0, dtype=jnp.float32)
            cql_optimizer = optax.adam(learning_rate=3e-4)
            cql_optimizer_state = cql_optimizer.init(log_cql_alpha)
        else:
            if cql_lagrange_threshold:
                raise ValueError(
                    "cql_lagrange_threshold should not be set when "
                    "fixed_cql_coefficient is provided"
                )

        def alpha_loss(
            log_sac_alpha: jnp.ndarray,
            policy_params: networks_lib.Params,
            transitions: types.Transition,
            key: jnp.ndarray,
        ) -> jnp.ndarray:
            """Eq 18 from https://arxiv.org/pdf/1812.05905.pdf."""
            dist_params = networks.policy_network.apply(
                policy_params, transitions.observation
            )
            action = networks.sample(dist_params, key)
            log_prob = networks.log_prob(dist_params, action)
            sac_alpha = jnp.exp(log_sac_alpha)
            sac_alpha_loss = sac_alpha * jax.lax.stop_gradient(
                -log_prob - target_entropy
            )
            return jnp.mean(sac_alpha_loss)

        def sac_critic_loss(
            q_old_action: jnp.ndarray,
            policy_params: networks_lib.Params,
            target_critic_params: networks_lib.Params,
            transitions: types.Transition,
            key: networks_lib.PRNGKey,
        ) -> jnp.ndarray:
            """Computes the SAC part of the loss."""
            next_dist_params = networks.policy_network.apply(
                policy_params, transitions.next_observation
            )
            next_action = networks.sample(next_dist_params, key)
            next_q = networks.critic_network.apply(
                target_critic_params, transitions.next_observation, next_action
            )
            next_v = jnp.min(next_q, axis=-1)
            target_q = jax.lax.stop_gradient(
                transitions.reward * reward_scale
                + transitions.discount * discount * next_v
            )
            return jnp.mean(jnp.square(q_old_action - jnp.expand_dims(target_q, -1)))

        def batched_critic(
            actions: jnp.ndarray,
            critic_params: networks_lib.Params,
            observation: jnp.ndarray,
        ) -> jnp.ndarray:
            """Applies the critic network to a batch of sampled actions."""
            actions = jax.lax.stop_gradient(actions)
            tiled_actions = jnp.reshape(actions, (batch_size * cql_num_samples, -1))
            tiled_states = jnp.tile(observation, [cql_num_samples, 1])
            tiled_q = networks.critic_network.apply(
                critic_params, tiled_states, tiled_actions
            )
            return jnp.reshape(tiled_q, (cql_num_samples, batch_size, -1))

        def cql_critic_loss(
            q_old_action: jnp.ndarray,
            critic_params: networks_lib.Params,
            policy_params: networks_lib.Params,
            transitions: types.Transition,
            key: networks_lib.PRNGKey,
        ) -> jnp.ndarray:
            """Computes the CQL part of the loss."""
            # The CQL part of the loss is
            #     logsumexp(Q(s,Â·)) - Q(s,a),
            # where s is the currrent state, and a the action in the dataset (so
            # Q(s,a) is simply q_old_action.
            # We need to estimate logsumexp(Q). This is done with importance sampling
            # (IS). This function implements the unlabeled equation page 29, Appx. F,
            # in https://arxiv.org/abs/2006.04779.
            # Here, IS is done with the uniform distribution and the policy in the
            # current state s. In their implementation, the authors also add the
            # policy in the transiting state s':
            # https://github.com/aviralkumar2907/CQL/blob/master/d4rl/rlkit/torch/sac/cql.py,
            # (l. 233-236).

            key_policy, key_policy_next, key_uniform = jax.random.split(key, 3)

            def sampled_q(obs, key):
                actions, log_probs = apply_and_sample_n(
                    key, networks, policy_params, obs, cql_num_samples
                )
                return batched_critic(
                    actions, critic_params, transitions.observation
                ) - jax.lax.stop_gradient(jnp.expand_dims(log_probs, -1))

            # Sample wrt policy in s
            sampled_q_from_policy = sampled_q(transitions.observation, key_policy)

            # Sample wrt policy in s'
            sampled_q_from_policy_next = sampled_q(
                transitions.next_observation, key_policy_next
            )

            # Sample wrt uniform
            actions_uniform = jax.random.uniform(
                key_uniform,
                (cql_num_samples, batch_size) + action_spec.shape,
                minval=action_spec.minimum,
                maxval=action_spec.maximum,
            )
            log_prob_uniform = -jnp.sum(
                jnp.log(action_spec.maximum - action_spec.minimum)
            )
            sampled_q_from_uniform = (
                batched_critic(actions_uniform, critic_params, transitions.observation)
                - log_prob_uniform
            )

            # Combine the samplings
            combined = jnp.concatenate(
                (
                    sampled_q_from_uniform,
                    sampled_q_from_policy,
                    sampled_q_from_policy_next,
                ),
                axis=0,
            )
            lse_q = jax.nn.logsumexp(combined, axis=0, b=1.0 / (3 * cql_num_samples))

            return jnp.mean(lse_q - q_old_action)

        def critic_loss(
            critic_params: networks_lib.Params,
            policy_params: networks_lib.Params,
            target_critic_params: networks_lib.Params,
            cql_alpha: jnp.ndarray,
            transitions: types.Transition,
            key: networks_lib.PRNGKey,
        ) -> jnp.ndarray:
            """Computes the full critic loss."""
            key_cql, key_sac = jax.random.split(key, 2)
            q_old_action = networks.critic_network.apply(
                critic_params, transitions.observation, transitions.action
            )
            cql_loss = cql_critic_loss(
                q_old_action, critic_params, policy_params, transitions, key_cql
            )
            sac_loss = sac_critic_loss(
                q_old_action, policy_params, target_critic_params, transitions, key_sac
            )
            return cql_alpha * cql_loss + sac_loss

        def cql_lagrange_loss(
            log_cql_alpha: jnp.ndarray,
            critic_params: networks_lib.Params,
            policy_params: networks_lib.Params,
            transitions: types.Transition,
            key: jnp.ndarray,
        ) -> jnp.ndarray:
            """Computes the loss that optimizes the cql coefficient."""
            cql_alpha = jnp.exp(log_cql_alpha)
            q_old_action = networks.critic_network.apply(
                critic_params, transitions.observation, transitions.action
            )
            return -cql_alpha * (
                cql_critic_loss(
                    q_old_action, critic_params, policy_params, transitions, key
                )
                - cql_lagrange_threshold
            )

        def actor_loss(
            policy_params: networks_lib.Params,
            critic_params: networks_lib.Params,
            sac_alpha: jnp.ndarray,
            transitions: types.Transition,
            key: jnp.ndarray,
            in_initial_bc_iters: bool,
        ) -> jnp.ndarray:
            """Computes the loss for the policy."""
            dist_params = networks.policy_network.apply(
                policy_params, transitions.observation
            )
            if in_initial_bc_iters:
                log_prob = networks.log_prob(dist_params, transitions.action)
                actor_loss = -jnp.mean(log_prob)
            else:
                action = networks.sample(dist_params, key)
                log_prob = networks.log_prob(dist_params, action)
                q_action = networks.critic_network.apply(
                    critic_params, transitions.observation, action
                )
                min_q = jnp.min(q_action, axis=-1)
                actor_loss = jnp.mean(sac_alpha * log_prob - min_q)
            return actor_loss

        alpha_grad = jax.value_and_grad(alpha_loss)
        cql_lagrange_grad = jax.value_and_grad(cql_lagrange_loss)
        critic_grad = jax.value_and_grad(critic_loss)
        actor_grad = jax.value_and_grad(actor_loss)

        def update_step(
            state: TrainingState,
            rb_transitions: types.Transition,
            in_initial_bc_iters: bool,
        ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:

            key, key_alpha, key_critic, key_actor = jax.random.split(state.key, 4)

            if adaptive_entropy_coefficient:
                alpha_loss, alpha_grads = alpha_grad(
                    state.log_sac_alpha, state.policy_params, rb_transitions, key_alpha
                )
                sac_alpha = jnp.exp(state.log_sac_alpha)
            else:
                sac_alpha = fixed_entropy_coefficient

            if adaptive_cql_coefficient:
                cql_lagrange_loss, cql_lagrange_grads = cql_lagrange_grad(
                    state.log_cql_alpha,
                    state.critic_params,
                    state.policy_params,
                    rb_transitions,
                    key_critic,
                )
                cql_lagrange_grads = jnp.clip(
                    cql_lagrange_grads,
                    -_CQL_GRAD_CLIPPING_VALUE,
                    _CQL_GRAD_CLIPPING_VALUE,
                )
                cql_alpha = jnp.exp(state.log_cql_alpha)
                cql_alpha = jnp.clip(
                    cql_alpha, a_min=0.0, a_max=_CQL_COEFFICIENT_MAX_VALUE
                )
            else:
                cql_alpha = fixed_cql_coefficient

            critic_loss, critic_grads = critic_grad(
                state.critic_params,
                state.policy_params,
                state.target_critic_params,
                cql_alpha,
                rb_transitions,
                key_critic,
            )
            actor_loss, actor_grads = actor_grad(
                state.policy_params,
                state.critic_params,
                sac_alpha,
                rb_transitions,
                key_actor,
                in_initial_bc_iters,
            )

            # Apply policy gradients
            actor_update, policy_optimizer_state = policy_optimizer.update(
                actor_grads, state.policy_optimizer_state
            )
            policy_params = optax.apply_updates(state.policy_params, actor_update)

            # Apply critic gradients
            critic_update, critic_optimizer_state = critic_optimizer.update(
                critic_grads, state.critic_optimizer_state
            )
            critic_params = optax.apply_updates(state.critic_params, critic_update)

            new_target_critic_params = jax.tree_map(
                lambda x, y: x * (1 - tau) + y * tau,
                state.target_critic_params,
                critic_params,
            )

            metrics = {
                "critic_loss": critic_loss,
                "actor_loss": actor_loss,
            }

            new_state = TrainingState(
                policy_optimizer_state=policy_optimizer_state,
                critic_optimizer_state=critic_optimizer_state,
                policy_params=policy_params,
                critic_params=critic_params,
                target_critic_params=new_target_critic_params,
                key=key,
                alpha_optimizer_state=state.alpha_optimizer_state,
                log_sac_alpha=state.log_sac_alpha,
                steps=state.steps + 1,
            )
            if adaptive_entropy_coefficient and (not in_initial_bc_iters):
                # Apply sac_alpha gradients
                alpha_update, alpha_optimizer_state = alpha_optimizer.update(
                    alpha_grads, state.alpha_optimizer_state
                )
                log_sac_alpha = optax.apply_updates(state.log_sac_alpha, alpha_update)
                metrics.update(
                    {"alpha_loss": alpha_loss, "sac_alpha": jnp.exp(log_sac_alpha),}
                )
                new_state = new_state._replace(
                    alpha_optimizer_state=alpha_optimizer_state,
                    log_sac_alpha=log_sac_alpha,
                )
            else:
                metrics["alpha_loss"] = 0.0
                metrics["sac_alpha"] = fixed_cql_coefficient

            if adaptive_cql_coefficient:
                # Apply cql coeff gradients
                cql_update, cql_optimizer_state = cql_optimizer.update(
                    cql_lagrange_grads, state.cql_optimizer_state
                )
                log_cql_alpha = optax.apply_updates(state.log_cql_alpha, cql_update)
                metrics.update(
                    {
                        "cql_lagrange_loss": cql_lagrange_loss,
                        "cql_alpha": jnp.exp(log_cql_alpha),
                    }
                )
                new_state = new_state._replace(
                    cql_optimizer_state=cql_optimizer_state, log_cql_alpha=log_cql_alpha
                )

            return new_state, metrics

        # General learner book-keeping and loggers.
        self._counter = counter or counting.Counter()
        self._logger = logger or loggers.make_default_logger(
            "learner",
            asynchronous=True,
            serialize_fn=utils.fetch_devicearray,
            steps_key=self._counter.get_steps_key(),
        )

        # Iterator on demonstration transitions.
        self._demonstrations = demonstrations

        # Use the JIT compiler.
        update_step_in_initial_bc_iters = utils.process_multiple_batches(
            lambda x, y: update_step(x, y, True), num_sgd_steps_per_step
        )
        update_step_rest = utils.process_multiple_batches(
            lambda x, y: update_step(x, y, False), num_sgd_steps_per_step
        )

        self._update_step_in_initial_bc_iters = jax.jit(update_step_in_initial_bc_iters)
        self._update_step_rest = jax.jit(update_step_rest)

        # Create initial state.
        key_policy, key_q, training_state_key = jax.random.split(random_key, 3)
        del random_key
        policy_params = networks.policy_network.init(key_policy)
        policy_optimizer_state = policy_optimizer.init(policy_params)
        critic_params = networks.critic_network.init(key_q)
        critic_optimizer_state = critic_optimizer.init(critic_params)

        self._state = TrainingState(
            policy_optimizer_state=policy_optimizer_state,
            critic_optimizer_state=critic_optimizer_state,
            policy_params=policy_params,
            critic_params=critic_params,
            target_critic_params=critic_params,
            key=training_state_key,
            steps=0,
        )

        if adaptive_entropy_coefficient:
            self._state = self._state._replace(
                alpha_optimizer_state=alpha_optimizer_state, log_sac_alpha=log_sac_alpha
            )
        if adaptive_cql_coefficient:
            self._state = self._state._replace(
                cql_optimizer_state=cql_optimizer_state, log_cql_alpha=log_cql_alpha
            )

        # Do not record timestamps until after the first learning step is done.
        # This is to avoid including the time it takes for actors to come online and
        # fill the replay buffer.
        self._timestamp = None

    def step(self):
        # Get data from replay (dropping extras if any). Note there is no
        # extra data here because we do not insert any into Reverb.
        transitions = next(self._demonstrations)

        counts = self._counter.get_counts()
        if "learner_steps" not in counts:
            cur_step = 0
        else:
            cur_step = counts["learner_steps"]
        in_initial_bc_iters = cur_step < self._num_bc_iters

        if in_initial_bc_iters:
            self._state, metrics = self._update_step_in_initial_bc_iters(
                self._state, transitions
            )
        else:
            self._state, metrics = self._update_step_rest(self._state, transitions)

        # Compute elapsed time.
        timestamp = time.time()
        elapsed_time = timestamp - self._timestamp if self._timestamp else 0
        self._timestamp = timestamp

        # Increment counts and record the current time
        counts = self._counter.increment(steps=1, walltime=elapsed_time)

        # Attempts to write the logs.
        self._logger.write({**metrics, **counts})

    def get_variables(self, names: List[str]) -> List[Any]:
        variables = {
            "policy": self._state.policy_params,
        }
        return [variables[name] for name in names]

    def save(self) -> TrainingState:
        return self._state

    def restore(self, state: TrainingState):
        self._state = state
