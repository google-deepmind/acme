{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULdrhOaVbsdO"
      },
      "source": [
        "# Acme: Quickstart\n",
        "## Guide to installing Acme and training your first D4PG agent.\n",
        "# <a href=\"https://colab.research.google.com/github/deepmind/acme/blob/master/examples/quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogw2P040-F5P"
      },
      "source": [
        "## Select your environment library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xflSXJPS8Qpm"
      },
      "outputs": [],
      "source": [
        "environment_library = 'gym'  # @param ['dm_control', 'gym']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaJxoatMhJ71"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovuCuHCC78Zu"
      },
      "source": [
        "### Install Acme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "KH3O0zcXUeun"
      },
      "outputs": [],
      "source": [
        "!pip install dm-acme[tf,jax]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEEj3Qw60y73"
      },
      "source": [
        "### Install the environment library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbZxYDxzoz5R"
      },
      "outputs": [],
      "source": [
        "if environment_library == 'dm_control':\n",
        "  import distutils.util\n",
        "  import subprocess\n",
        "  if subprocess.run('nvidia-smi').returncode:\n",
        "    raise RuntimeError(\n",
        "        'Cannot communicate with GPU. '\n",
        "        'Make sure you are using a GPU Colab runtime. '\n",
        "        'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "  mujoco_dir = \"$HOME/.mujoco\"\n",
        "\n",
        "  print('Installing OpenGL dependencies...')\n",
        "  !apt-get update -qq\n",
        "  !apt-get install -qq -y --no-install-recommends libglew2.0 > /dev/null\n",
        "\n",
        "  print('Downloading MuJoCo...')\n",
        "  BASE_URL = 'https://github.com/deepmind/mujoco/releases/download'\n",
        "  MUJOCO_VERSION = '2.1.1'\n",
        "  MUJOCO_ARCHIVE = (\n",
        "      f'mujoco-{MUJOCO_VERSION}-{distutils.util.get_platform()}.tar.gz')\n",
        "  !wget -q \"{BASE_URL}/{MUJOCO_VERSION}/{MUJOCO_ARCHIVE}\"\n",
        "  !wget -q \"{BASE_URL}/{MUJOCO_VERSION}/{MUJOCO_ARCHIVE}.sha256\"\n",
        "  check_result = !shasum -c \"{MUJOCO_ARCHIVE}.sha256\"\n",
        "  if _exit_code:\n",
        "    raise RuntimeError(\n",
        "        'Downloaded MuJoCo archive is corrupted (checksum mismatch)')\n",
        "\n",
        "  print('Unpacking MuJoCo...')\n",
        "  MUJOCO_DIR = '$HOME/.mujoco'\n",
        "  !mkdir -p \"{MUJOCO_DIR}\"\n",
        "  !tar -zxf {MUJOCO_ARCHIVE} -C \"{MUJOCO_DIR}\"\n",
        "\n",
        "  # Configure dm_control to use the EGL rendering backend (requires GPU)\n",
        "  %env MUJOCO_GL=egl\n",
        "\n",
        "  print('Installing dm_control...')\n",
        "  # Version 0.0.416848645 is the first one to support MuJoCo 2.1.1.\n",
        "  !pip install -q dm_control>=0.0.416848645\n",
        "\n",
        "  print('Checking that the dm_control installation succeeded...')\n",
        "  try:\n",
        "    from dm_control import suite\n",
        "    env = suite.load('cartpole', 'swingup')\n",
        "    pixels = env.physics.render()\n",
        "  except Exception as e:\n",
        "    raise e from RuntimeError(\n",
        "        'Something went wrong during installation. Check the shell output above '\n",
        "        'for more information.\\n'\n",
        "        'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "        'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "  else:\n",
        "    del suite, env, pixels\n",
        "\n",
        "  !echo Installed dm_control $(pip show dm_control | grep -Po \"(?<=Version: ).+\")\n",
        "\n",
        "elif environment_library == 'gym':\n",
        "  !pip install gym[classic_control]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl8eyWblSs-z"
      },
      "source": [
        "### Install visualization packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSM7KHDFSsQS"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install imageio[ffmpeg]\n",
        "!pip install PILLOW\n",
        "!pip install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-H2d6UZi7Sf"
      },
      "source": [
        "## Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "HJ74Id-8MERq"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "\n",
        "from acme import environment_loop\n",
        "from acme import specs\n",
        "from acme import wrappers\n",
        "from acme.agents.tf import d4pg\n",
        "from acme.tf import networks\n",
        "from acme.tf import utils as tf2_utils\n",
        "from acme.utils import loggers\n",
        "import numpy as np\n",
        "import sonnet as snt\n",
        "\n",
        "# Import the selected environment lib\n",
        "if environment_library == 'dm_control':\n",
        "  from dm_control import suite\n",
        "elif environment_library == 'gym':\n",
        "  import gym\n",
        "\n",
        "# Imports required for visualization\n",
        "import pyvirtualdisplay\n",
        "import imageio\n",
        "import base64\n",
        "\n",
        "# Set up a virtual display for rendering.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6KuVGSk4uc9"
      },
      "source": [
        "## Load an environment\n",
        "\n",
        "We can now load an environment. In what follows we'll create an environment and grab the environment's specifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "4PVlHtGF5yzt"
      },
      "outputs": [],
      "source": [
        "if environment_library == 'dm_control':\n",
        "  environment = suite.load('cartpole', 'balance')\n",
        "  \n",
        "elif environment_library == 'gym':\n",
        "  environment = gym.make('MountainCarContinuous-v0')\n",
        "  environment = wrappers.GymWrapper(environment)  # To dm_env interface.\n",
        "\n",
        "else:\n",
        "  raise ValueError(\n",
        "      \"Unknown environment library: {};\".format(environment_library) +\n",
        "      \"choose among ['dm_control', 'gym'].\")\n",
        "\n",
        "# Make sure the environment outputs single-precision floats.\n",
        "environment = wrappers.SinglePrecisionWrapper(environment)\n",
        "\n",
        "# Grab the spec of the environment.\n",
        "environment_spec = specs.make_environment_spec(environment)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BukOfOsmtSQn"
      },
      "source": [
        " ## Create a D4PG agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Jcjk1w6oHVX"
      },
      "outputs": [],
      "source": [
        "#@title Build agent networks\n",
        "\n",
        "# Get total number of action dimensions from action spec.\n",
        "num_dimensions = np.prod(environment_spec.actions.shape, dtype=int)\n",
        "\n",
        "# Create the shared observation network; here simply a state-less operation.\n",
        "observation_network = tf2_utils.batch_concat\n",
        "\n",
        "# Create the deterministic policy network.\n",
        "policy_network = snt.Sequential([\n",
        "    networks.LayerNormMLP((256, 256, 256), activate_final=True),\n",
        "    networks.NearZeroInitializedLinear(num_dimensions),\n",
        "    networks.TanhToSpec(environment_spec.actions),\n",
        "])\n",
        "\n",
        "# Create the distributional critic network.\n",
        "critic_network = snt.Sequential([\n",
        "    # The multiplexer concatenates the observations/actions.\n",
        "    networks.CriticMultiplexer(),\n",
        "    networks.LayerNormMLP((512, 512, 256), activate_final=True),\n",
        "    networks.DiscreteValuedHead(vmin=-150., vmax=150., num_atoms=51),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CD2sNK-oA9S"
      },
      "outputs": [],
      "source": [
        "# Create a logger for the agent and environment loop.\n",
        "agent_logger = loggers.TerminalLogger(label='agent', time_delta=10.)\n",
        "env_loop_logger = loggers.TerminalLogger(label='env_loop', time_delta=10.)\n",
        "\n",
        "# Create the D4PG agent.\n",
        "agent = d4pg.D4PG(\n",
        "    environment_spec=environment_spec,\n",
        "    policy_network=policy_network,\n",
        "    critic_network=critic_network,\n",
        "    observation_network=observation_network,\n",
        "    sigma=1.0,\n",
        "    logger=agent_logger,\n",
        "    checkpoint=False\n",
        ")\n",
        "\n",
        "# Create an loop connecting this agent to the environment created above.\n",
        "env_loop = environment_loop.EnvironmentLoop(\n",
        "    environment, agent, logger=env_loop_logger)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKeGQxzitXYC"
      },
      "source": [
        "## Run a training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWZd5N-Qoz82"
      },
      "outputs": [],
      "source": [
        "# Run a `num_episodes` training episodes.\n",
        "# Rerun this cell until the agent has learned the given task.\n",
        "env_loop.run(num_episodes=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do57Ql4ZsWDu"
      },
      "source": [
        "## Visualize an evaluation loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJXkfg6LSZ-h"
      },
      "source": [
        "### Helper functions for rendering and vizualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIJRbtAlxQVu"
      },
      "outputs": [],
      "source": [
        "# Create a simple helper function to render a frame from the current state of\n",
        "# the environment.\n",
        "if environment_library == 'dm_control':\n",
        "  def render(env):\n",
        "    return env.physics.render(camera_id=0)\n",
        "elif environment_library == 'gym':\n",
        "  def render(env):\n",
        "    return env.environment.render(mode='rgb_array')\n",
        "else:\n",
        "  raise ValueError(\n",
        "      \"Unknown environment library: {};\".format(environment_library) +\n",
        "      \"choose among ['dm_control', 'gym'].\")\n",
        "\n",
        "def display_video(frames, filename='temp.mp4'):\n",
        "  \"\"\"Save and display video.\"\"\"\n",
        "\n",
        "  # Write video\n",
        "  with imageio.get_writer(filename, fps=60) as video:\n",
        "    for frame in frames:\n",
        "      video.append_data(frame)\n",
        "\n",
        "  # Read video and display the video\n",
        "  video = open(filename, 'rb').read()\n",
        "  b64_video = base64.b64encode(video)\n",
        "  video_tag = ('<video  width=\"320\" height=\"240\" controls alt=\"test\" '\n",
        "               'src=\"data:video/mp4;base64,{0}\">').format(b64_video.decode())\n",
        "\n",
        "  return IPython.display.HTML(video_tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh8x46UKSf2c"
      },
      "source": [
        "### Run and visualize the agent in the environment for an episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2__mFiraWND1"
      },
      "outputs": [],
      "source": [
        "timestep = environment.reset()\n",
        "frames = [render(environment)]\n",
        "\n",
        "while not timestep.last():\n",
        "  # Simple environment loop.\n",
        "  action = agent.select_action(timestep.observation)\n",
        "  timestep = environment.step(action)\n",
        "\n",
        "  # Render the scene and add it to the frame stack.\n",
        "  frames.append(render(environment))\n",
        "\n",
        "# Save and display a video of the behaviour.\n",
        "display_video(np.array(frames))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Acme: Quickstart",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
